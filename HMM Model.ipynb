{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shakka/anaconda3/envs/cs224u/lib/python3.6/site-packages/dicom/__init__.py:53: UserWarning: \n",
      "This code is using an older version of pydicom, which is no longer \n",
      "maintained as of Jan 2017.  You can access the new pydicom features and API \n",
      "by installing `pydicom` from PyPI.\n",
      "See 'Transitioning to pydicom 1.x' section at pydicom.readthedocs.org \n",
      "for more information.\n",
      "\n",
      "  warnings.warn(msg)\n",
      "/home/shakka/anaconda3/envs/cs224u/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hmmlearn import hmm\n",
    "\n",
    "import cardio.dataset as ds\n",
    "from cardio import EcgBatch\n",
    "from cardio.dataset import B, V, F\n",
    "from cardio.models.hmm import HMModel, prepare_hmm_input\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annsamples(batch):\n",
    "    return [ann[\"annsamp\"] for ann in batch.annotation]\n",
    "\n",
    "def get_anntypes(batch):\n",
    "    return [ann[\"anntype\"] for ann in batch.annotation]\n",
    "\n",
    "def expand_annotation(annsamp, anntype, length):\n",
    "    \"\"\"Unravel annotation\n",
    "    \"\"\"\n",
    "    begin = -1\n",
    "    end = -1\n",
    "    s = 'none'\n",
    "    states = {'N':0, 'st':1, 't':2, 'iso':3, 'p':4, 'pq':5}\n",
    "    annot_expand = -1 * np.ones(length)\n",
    "\n",
    "    for j, samp in enumerate(annsamp):\n",
    "        if anntype[j] == '(':\n",
    "            begin = samp\n",
    "            if (end > 0) & (s != 'none'):\n",
    "                if s == 'N':\n",
    "                    annot_expand[end:begin] = states['st']\n",
    "                elif s == 't':\n",
    "                    annot_expand[end:begin] = states['iso']\n",
    "                elif s == 'p':\n",
    "                    annot_expand[end:begin] = states['pq']\n",
    "        elif anntype[j] == ')':\n",
    "            end = samp\n",
    "            if (begin > 0) & (s != 'none'):\n",
    "                annot_expand[begin:end] = states[s]\n",
    "        else:\n",
    "            s = anntype[j]\n",
    "\n",
    "    return annot_expand\n",
    "\n",
    "def prepare_means_covars(hmm_features, clustering, states=[3, 5, 11, 14, 17, 19], num_states=19, num_features=3):\n",
    "    \"\"\"This function is specific to the task and the model configuration, thus contains hardcode.\n",
    "    \"\"\"\n",
    "    means = np.zeros((num_states, num_features))\n",
    "    covariances = np.zeros((num_states, num_features, num_features))\n",
    "    \n",
    "    # Prepearing means and variances\n",
    "    last_state = 0\n",
    "    unique_clusters = len(np.unique(clustering)) - 1 # Excuding value -1, which represents undefined state\n",
    "    for state, cluster in zip(states, np.arange(unique_clusters)):\n",
    "        value = hmm_features[clustering == cluster, :]\n",
    "        means[last_state:state, :] = np.mean(value, axis=0)\n",
    "        covariances[last_state:state, :, :] = value.T.dot(value) / np.sum(clustering == cluster)\n",
    "        last_state = state\n",
    "        \n",
    "    return means, covariances\n",
    "\n",
    "def prepare_transmat_startprob():\n",
    "    \"\"\" This function is specific to the task and the model configuration, thus contains hardcode.\n",
    "    \"\"\"\n",
    "    # Transition matrix - each row should add up tp 1\n",
    "    transition_matrix = np.diag(19 * [14/15.0]) + np.diagflat(18 * [1/15.0], 1) + np.diagflat([1/15.0], -18)\n",
    "    \n",
    "    # We suppose that absence of P-peaks is possible\n",
    "    transition_matrix[13,14]=0.9*1/15.0\n",
    "    transition_matrix[13,17]=0.1*1/15.0\n",
    "\n",
    "    # Initial distribution - should add up to 1\n",
    "    start_probabilities = np.array(19 * [1/np.float(19)])\n",
    "    \n",
    "    return transition_matrix, start_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SIGNALS_PATH = \"hmm_data\"\n",
    "SIGNALS_MASK = os.path.join(SIGNALS_PATH, \"*.hea\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ds.FilesIndex(path=SIGNALS_MASK, no_ext=True, sort=True)\n",
    "dtst = ds.Dataset(index, batch_class=EcgBatch)\n",
    "dtst.cv_split(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify optimal starting params\n",
    "template_ppl_inits = (\n",
    "    ds.Pipeline()\n",
    "      .init_variable(\"annsamps\", init_on_each_run=list)\n",
    "      .init_variable(\"anntypes\", init_on_each_run=list)\n",
    "      .init_variable(\"hmm_features\", init_on_each_run=list)\n",
    "      .load(fmt='wfdb', components=[\"signal\", \"annotation\", \"meta\"], ann_ext='pu1')\n",
    "      .cwt(src=\"signal\", dst=\"hmm_features\", scales=[4,8,16], wavelet=\"mexh\")\n",
    "      .standardize(axis=-1, src=\"hmm_features\", dst=\"hmm_features\")\n",
    "      .update_variable(\"annsamps\", ds.F(get_annsamples), mode='e')\n",
    "      .update_variable(\"anntypes\", ds.F(get_anntypes), mode='e')\n",
    "      .update_variable(\"hmm_features\", ds.B(\"hmm_features\"), mode='e')\n",
    "      .run(batch_size=20, shuffle=False, drop_last=False, n_epochs=1, lazy=True)\n",
    ")\n",
    "\n",
    "ppl_inits = (dtst >> template_ppl_inits).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [hmm_features.shape[2] for hmm_features in ppl_inits.get_variable(\"hmm_features\")]\n",
    "hmm_features = np.concatenate([hmm_features[0,:,:].T for hmm_features in ppl_inits.get_variable(\"hmm_features\")])\n",
    "anntype = ppl_inits.get_variable(\"anntypes\")\n",
    "annsamp = ppl_inits.get_variable(\"annsamps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded = np.concatenate([expand_annotation(samp, types, length) for samp, types, length in zip(annsamp, anntype, lengths)])\n",
    "means, covariances = prepare_means_covars(hmm_features, expanded, states = [3, 5, 11, 14, 17, 19], num_features = 3)\n",
    "transition_matrix, start_probabilities = prepare_transmat_startprob()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training of model\n",
    "config_train = {\n",
    "    'build': True,\n",
    "    'estimator': hmm.GaussianHMM(n_components=19, n_iter=25, covariance_type=\"full\", random_state=42,\n",
    "                                 init_params='', verbose=False),\n",
    "    'init_params': {'means_': means, 'covars_': covariances, 'transmat_': transition_matrix,\n",
    "                    'startprob_': start_probabilities}\n",
    "                }\n",
    "\n",
    "ppl_train_template = (\n",
    "    ds.Pipeline()\n",
    "      .init_model(\"dynamic\", HMModel, \"HMM\", config=config_train)\n",
    "      .load(fmt='wfdb', components=[\"signal\", \"annotation\", \"meta\"], ann_ext='pu1')\n",
    "      .cwt(src=\"signal\", dst=\"hmm_features\", scales=[4,8,16], wavelet=\"mexh\")\n",
    "      .standardize(axis=-1, src=\"hmm_features\", dst=\"hmm_features\")\n",
    "      .train_model(\"HMM\", make_data=partial(prepare_hmm_input, features=\"hmm_features\", channel_ix=0))\n",
    "      .run(batch_size=10, shuffle=False, drop_last=False, n_epochs=1, lazy=True)\n",
    ")\n",
    "\n",
    "# ppl_train = (\n",
    "#   dtst.train\n",
    "#       .pipeline  \n",
    "#       .init_model(\"dynamic\", HMModel, \"HMM\", config=config_train)\n",
    "#       .load(fmt='wfdb', components=[\"signal\", \"annotation\", \"meta\"], ann_ext='pu1')\n",
    "#       .wavelet_transform_signal(cwt_scales=[4,8,16], cwt_wavelet=\"mexh\")\n",
    "#       .train_model(\"HMM\", make_data=prepare_batch)\n",
    "#       .run(batch_size=20, shuffle=False, drop_last=False, n_epochs=1)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_train = (dtst >> ppl_train_template).run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_train.save_model(\"HMM\", path=\"./models/hmmodel.dill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
